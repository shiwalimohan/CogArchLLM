{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-14T19:01:41.867984Z",
     "start_time": "2025-03-14T19:01:41.863504Z"
    }
   },
   "source": [
    "import scipy.stats\n",
    "from openai import OpenAI\n",
    "from math import exp\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import entropy\n",
    "import math\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import ast\n",
    "np.set_printoptions(legacy='1.25')\n",
    "import pprint\n"
   ],
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:16:55.779936Z",
     "start_time": "2025-03-13T21:16:55.739987Z"
    }
   },
   "cell_type": "code",
   "source": "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
   "id": "363810ed82a291ee",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### GPT methods",
   "id": "bd7f0773f15df0fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T21:16:59.018530Z",
     "start_time": "2025-03-13T21:16:59.014180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ],
   "id": "67a36d546259d43",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T19:06:07.632175Z",
     "start_time": "2025-03-14T19:06:07.629143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_ans_token_logprobs(response, anskey):\n",
    "    tokenprobs = response.choices[0].logprobs.content\n",
    "    i = 0\n",
    "    token = tokenprobs[i].token.strip()\n",
    "    #print(i, token)\n",
    "    while token != anskey:\n",
    "        i += 1\n",
    "        token = tokenprobs[i].token.strip()\n",
    "        #print('forward {}, {}'.format(i, token))\n",
    "\n",
    "    while token != '{' and token != '{\\'' and token != '{\\\"':\n",
    "        i -= 1\n",
    "        token = tokenprobs[i].token.strip()\n",
    "        #print('backward {}, {}'.format(i, token))\n",
    "\n",
    "    json_begin = i\n",
    "    #print('json_begins at {}'.format(json_begin))\n",
    "\n",
    "    while token != '}' and token != '}\\'' and token != '}.' and token != '\\'}' and token != '}\\\"':\n",
    "        i += 1\n",
    "        token = tokenprobs[i].token.strip()\n",
    "        #print('forward {}, {}'.format(i, token))\n",
    "\n",
    "    json_end = i\n",
    "    #print(json_begin, json_end)\n",
    "    response_list = tokenprobs[json_begin: json_end+1]\n",
    "    response_string = \"\".join([o.token for o in response_list]).strip('.')\n",
    "    response_dict = ast.literal_eval(response_string)\n",
    "\n",
    "    ans_logprobs = None\n",
    "\n",
    "    #print(str(response_dict[anskey]))\n",
    "    for item in response_list:\n",
    "        if item.token == str(response_dict[anskey]):\n",
    "            ans_logprobs = item.top_logprobs\n",
    "\n",
    "\n",
    "    return response_dict, ans_logprobs\n",
    "\n"
   ],
   "id": "50a9a8ae759cbf0b",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T20:34:37.741290Z",
     "start_time": "2025-03-14T20:34:37.737915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_answer(question, prompt, anskey):\n",
    "    num_next_tokens = 5\n",
    "    prompt_up = prompt.format(question=question)\n",
    "    #print(prompt_up)\n",
    "    API_RESPONSE = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": prompt_up}],\n",
    "        model=\"gpt-4\",\n",
    "        logprobs=True,\n",
    "        top_logprobs=num_next_tokens,\n",
    "    )\n",
    "\n",
    "    response_dict, ans_logprobs = get_ans_token_logprobs(API_RESPONSE, anskey)\n",
    "\n",
    "    alters = []\n",
    "    probs = []\n",
    "    for item in ans_logprobs:\n",
    "        linear_prob = np.round(np.exp(item.logprob)*100, 3)\n",
    "        alters.append((item.token, linear_prob, item.logprob))\n",
    "        probs.append(linear_prob)\n",
    "\n",
    "\n",
    "    return_dict = {\n",
    "        '1. prompt': prompt_up,\n",
    "        '2. response': API_RESPONSE.choices[0].message.content,\n",
    "        '3. response_dict': response_dict,\n",
    "        '4. entropy': np.round(scipy.stats.entropy(probs, base=2)/scipy.stats.entropy([0.2, 0.2, 0.2, 0.2, 0.2], base=2), 3),\n",
    "        '5. alternatives': alters\n",
    "    }\n",
    "\n",
    "    return return_dict\n"
   ],
   "id": "81922087e1e256b1",
   "outputs": [],
   "execution_count": 290
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Prompts",
   "id": "31a60bf9c93f580d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T20:31:19.539741Z",
     "start_time": "2025-03-14T20:31:19.531139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "basePROMPT = \"\"\"You will be given a math word problem. Provide a numeric answer to the problem. MAKE SURE to only provide a numeric answer. Your final response should be a dictionary with key 'ans'.\n",
    "Problem: {question}\"\"\""
   ],
   "id": "18c9dc0893e33448",
   "outputs": [],
   "execution_count": 281
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T21:03:51.948331Z",
     "start_time": "2025-03-14T21:03:51.946300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "confidencePROMPT = \"\"\"You will be given a math word problem. Provide a numeric answer to the problem. MAKE SURE to only provide a numeric answer. Your confidence is how much you trust your answer to be correct and is on a scale of 1-10. Your final response should be a dictionary with key 'ans' and 'confidence'.\n",
    "Problem: {question}\"\"\""
   ],
   "id": "4e15c12645fc6ef4",
   "outputs": [],
   "execution_count": 310
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:54:49.472182Z",
     "start_time": "2025-03-14T16:54:49.470508Z"
    }
   },
   "cell_type": "code",
   "source": "emptyPROMPT=\"\"\"Your final response should be a dictionary with key 'ans'. Problem: {question}\"\"\"",
   "id": "a42d07d7575c4aea",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T20:43:24.571632Z",
     "start_time": "2025-03-14T20:43:24.569602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cotPROMPT=\"\"\"\"You will be given a math word problem. Provide a numeric answer to the problem. Think step by step. Your final response should be a dictionary with key 'ans'.\n",
    "Problem: {question}\"\"\""
   ],
   "id": "6c294430b7de199",
   "outputs": [],
   "execution_count": 304
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Problem text",
   "id": "2e99dffa829a9566"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LLMs, particularly those with reasoning, are known to be good at solving algebra word problems. Here, I experiment with various levels of ambiguity in the question. The following problems are variations of the same type. The first two problems have an unambiguous answer that can be calculated in a straight forward fashion. The third problem doesn't explicitly specify which fight the question is about. The fourth problem requires additional layers of reasoning.",
   "id": "7fa373698523ba0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T20:41:40.797106Z",
     "start_time": "2025-03-14T20:41:40.794729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "problems = {\n",
    "            \"straight_opt1_01\": {\n",
    "                    \"question\": \"On average Joe throws 25 punches per minute. First fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds of 5 minutes. How many punches did he throw in the first fight?\",\n",
    "                    \"correct_ans\": 375,\n",
    "                    \"category\": \"unambiguous\"\n",
    "                },\n",
    "            \"straight_opt2_01\":{\n",
    "                    \"question\": \"On average Joe throws 25 punches per minute. First fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds of 5 minutes. How many punches did he throw in the second fight?\",\n",
    "                    \"correct_ans\": 500,\n",
    "                    \"category\": \"unambiguous\"\n",
    "                },\n",
    "            \"ambiguous_01\": {\n",
    "                    \"question\": \"On average Joe throws 25 punches per minute. First fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds of 5 minutes. How many punches did he throw in the fight?\",\n",
    "                    \"correct_ans\": None,\n",
    "                    \"category\": \"ambiguous\"\n",
    "                },\n",
    "            \"total_01\": {\n",
    "                    \"question\": \"On average Joe throws 25 punches per minute. First fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds of 5 minutes. How many punches did he throw in both fights?\",\n",
    "                    \"correct_ans\": 875,\n",
    "                    \"category\": \"unambiguous\"\n",
    "                }\n",
    "}"
   ],
   "id": "fb28ff628dc504ca",
   "outputs": [],
   "execution_count": 299
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Observations\n",
    "\n",
    "We begin with the easy, unambigous questions.\n"
   ],
   "id": "2644336b2d01eb96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T20:43:01.852560Z",
     "start_time": "2025-03-14T20:43:00.948263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ans = get_answer(question=problems['straight_opt1_01']['question'], prompt=basePROMPT, anskey='ans')\n",
    "pprint.pprint(ans)"
   ],
   "id": "4edef28d2b4ac0ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1. prompt': 'You will be given a math word problem. Provide a numeric answer '\n",
      "              'to the problem. MAKE SURE to only provide a numeric answer. '\n",
      "              \"Your final response should be a dictionary with key 'ans'.\\n\"\n",
      "              'Problem: On average Joe throws 25 punches per minute. First '\n",
      "              'fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds '\n",
      "              'of 5 minutes. How many punches did he throw in the first fight?',\n",
      " '2. response': \"{'ans': 375}\",\n",
      " '3. response_dict': {'ans': 375},\n",
      " '4. entropy': 0.0,\n",
      " '5. alternatives': [('375', 100.0, 0.0),\n",
      "                     ('325', 0.0, -17.845675),\n",
      "                     ('25', 0.0, -17.890102),\n",
      "                     ('75', 0.0, -18.251888),\n",
      "                     (' ', 0.0, -19.030909)]}\n"
     ]
    }
   ],
   "execution_count": 303
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T20:28:31.856960Z",
     "start_time": "2025-03-14T20:28:31.125234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ans = get_answer(question=problems['straight_opt2_01']['question'], prompt=basePROMPT, anskey='ans')\n",
    "pprint.pprint(ans)"
   ],
   "id": "3c6385460ef033e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be given a math word problem. Provide a numeric answer to the problem. Your final response should be a dictionary with key 'ans'.\n",
      "Problem: On average Joe throws 25 punches per minute. First fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds of 5 minutes. How many punches did he throw in the second fight?\n",
      "{'alternatives': [('500', 99.997, -3.4166656e-05),\n",
      "                  ('100', 0.001, -11.121738),\n",
      "                  ('200', 0.001, -11.738201),\n",
      "                  ('125', 0.0, -13.001865),\n",
      "                  ('400', 0.0, -13.233498)],\n",
      " 'entropy': 0.0,\n",
      " 'response': \"{'ans': 500}\",\n",
      " 'response_dict': {'ans': 500}}\n"
     ]
    }
   ],
   "execution_count": 277
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For both unambiguous questions, regular prompting can generate correct answers. I report entropy of the answer which captures the degree to which the LLM is 'confused' about the answer. In both these cases, there is no confusion and hence, the entropy is 0. All the other alternatives considered by the LLM have very low probabilities. This is what we expect when reasoning works as expected.",
   "id": "ce772303dc7e2244"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we consider the ambiguous question where the fight under consideration is not specified. We expect the entropy of the answer to be high, reflecting the LLMs is confused about the answer.",
   "id": "88ec1858b601bcf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T20:28:46.460597Z",
     "start_time": "2025-03-14T20:28:45.912510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ans = get_answer(question=problems['ambiguous_01']['question'], prompt=basePROMPT, anskey='ans')\n",
    "pprint.pprint(ans)"
   ],
   "id": "485161c90bc268fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be given a math word problem. Provide a numeric answer to the problem. Your final response should be a dictionary with key 'ans'.\n",
      "Problem: On average Joe throws 25 punches per minute. First fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds of 5 minutes. How many punches did he throw in the fight?\n",
      "{'alternatives': [('675', 35.515, -1.0352086),\n",
      "                  ('105', 15.977, -1.8340234),\n",
      "                  ('975', 9.613, -2.3420115),\n",
      "                  ('110', 4.68, -3.0617952),\n",
      "                  ('135', 4.216, -3.1663141)],\n",
      " 'entropy': 0.81,\n",
      " 'response': \"{'ans': 675}\",\n",
      " 'response_dict': {'ans': 675}}\n"
     ]
    }
   ],
   "execution_count": 278
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As expected, the entropy of the answer in very high reflecting the confusion within LLM's inference. However, the LLM does produce a response (675, which is incorrect). This behavior is very different from how a human would respond. Because the question is ambiguous, a human would ask 'which fight are you talking about?'. The secondary question elicits further information from the speaker and constrains ambiguity. LLMs cannot modulate their response based on 'confusion' in their inference systems.\n",
    "\n",
    "Next, I investigate if the LLM is aware of its own confusion. I asked it to report its own confidence on the answer on a scale of 1-10."
   ],
   "id": "bef9910c32116774"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T21:07:18.758574Z",
     "start_time": "2025-03-14T21:07:17.775318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ans = get_answer(question=problems['ambiguous_01']['question'], prompt=confidencePROMPT, anskey='ans')\n",
    "pprint.pprint(ans)"
   ],
   "id": "b95b852bbc2ddd86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1. prompt': 'You will be given a math word problem. Provide a numeric answer '\n",
      "              'to the problem. MAKE SURE to only provide a numeric answer. '\n",
      "              'Your confidence is how much you trust your answer to be correct '\n",
      "              'and is on a scale of 1-10. Your final response should be a '\n",
      "              \"dictionary with key 'ans' and 'confidence'.\\n\"\n",
      "              'Problem: On average Joe throws 25 punches per minute. First '\n",
      "              'fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds '\n",
      "              'of 5 minutes. How many punches did he throw in the fight?',\n",
      " '2. response': \"{'ans': 675, 'confidence': 10}\",\n",
      " '3. response_dict': {'ans': 675, 'confidence': 10},\n",
      " '4. entropy': 0.885,\n",
      " '5. alternatives': [('675', 23.18, -1.4618801),\n",
      "                     ('105', 22.48, -1.4925445),\n",
      "                     ('975', 17.125, -1.7646126),\n",
      "                     ('110', 4.887, -3.0185513),\n",
      "                     ('102', 4.453, -3.1116643)]}\n"
     ]
    }
   ],
   "execution_count": 315
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In its response, the LLM claims to be highly confident (with a score of 10). It reports that it is highly confident of the answer. However, not only is the answer wrong but high answer entropy suggests that the LLM is not confident in the answer.",
   "id": "bbbd5bd4fe3e20c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T21:06:46.885425Z",
     "start_time": "2025-03-14T21:06:45.951042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ans = get_answer(question=problems['total_01']['question'], prompt=basePROMPT, anskey='ans')\n",
    "problems['total_01']\n",
    "pprint.pprint(ans)"
   ],
   "id": "c71dd1562bd26d09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1. prompt': 'You will be given a math word problem. Provide a numeric answer '\n",
      "              'to the problem. MAKE SURE to only provide a numeric answer. '\n",
      "              \"Your final response should be a dictionary with key 'ans'.\\n\"\n",
      "              'Problem: On average Joe throws 25 punches per minute. First '\n",
      "              'fight lasts 5 rounds of 3 minutes. Second fight lasts 4 rounds '\n",
      "              'of 5 minutes. How many punches did he throw in both fights?',\n",
      " '2. response': \"{'ans': 675}\",\n",
      " '3. response_dict': {'ans': 675},\n",
      " '4. entropy': 0.496,\n",
      " '5. alternatives': [('675', 69.956, -0.3573107),\n",
      "                     ('975', 9.086, -2.398437),\n",
      "                     ('105', 5.728, -2.8598094),\n",
      "                     ('725', 3.057, -3.4875789),\n",
      "                     ('825', 1.902, -3.9623733)]}\n"
     ]
    }
   ],
   "execution_count": 313
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T20:42:54.748184Z",
     "start_time": "2025-03-14T20:42:54.746657Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b031b05d64122a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53d416f6115c6e58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
